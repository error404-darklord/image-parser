import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
save_folder = ""  # üëâ –ü–∞–ø–∫–∞ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ
os.makedirs(save_folder, exist_ok=True)

# === –í–í–û–î –ö–û–õ–ò–ß–ï–°–¢–í–ê –ü–†–û–¶–ï–°–°–û–í ===
try:
    num_urls = int(input("–°–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å? "))
except ValueError:
    print("–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —á–∏—Å–ª–æ.")
    exit()

# === –í–í–û–î –°–°–´–õ–û–ö ===
urls = []
for i in range(num_urls):
    u = input(f"[{i+1}] –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É: ").strip()
    urls.append(u)

# === –§–£–ù–ö–¶–ò–Ø –ü–ê–†–°–ò–ù–ì–ê –û–î–ù–û–ô –°–°–´–õ–ö–ò ===
def download_images_from_url(url, folder, base_count=0):
    print(f"\n[*] –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
    try:
        response = requests.get(url, timeout=10)
    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return 0

    soup = BeautifulSoup(response.text, "html.parser")
    images = soup.find_all("img")
    print(f"[*] –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")

    count = 0
    for img in images:
        src = img.get("src")
        if not src:
            continue

        img_url = urljoin(url, src)
        parsed = urlparse(img_url)
        ext = os.path.splitext(parsed.path)[1].lower()

        if ext not in [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp"]:
            print(f"[!] –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ): {img_url}")
            continue

        try:
            print(f"[>] –°–∫–∞—á–∏–≤–∞–µ–º: {img_url}")
            img_data = requests.get(img_url, timeout=5).content
            filename = os.path.join(folder, f"image_{base_count + count + 1}{ext}")
            with open(filename, "wb") as f:
                f.write(img_data)
            print(f"[+] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
            count += 1
        except Exception as e:
            print(f"[!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {img_url}: {e}")
    return count

# === –û–ë–†–ê–ë–û–¢–ö–ê –í–°–ï–• –°–°–´–õ–û–ö ===
total_downloaded = 0
for index, link in enumerate(urls):
    total_downloaded += download_images_from_url(link, save_folder, base_count=total_downloaded)

print(f"\n[‚úì] –ì–æ—Ç–æ–≤–æ! –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_downloaded}")
