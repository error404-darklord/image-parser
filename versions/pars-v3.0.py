import os
import requests
from colorama import init, Fore, Back, Style
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# MENU LOAD
def menu_load():
    P = "ENTER TO EXIT"
    MENU = """
                                                                
‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ      ‚ñà‚ñà     ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ  ‚ñÑ‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ  
  ‚ñà‚ñà   ‚ñÄ‚ñà‚ñà‚ñÑ    ‚ñÑ‚ñà‚ñà‚ñÑ      ‚ñà‚ñà   ‚ñÄ‚ñà‚ñà‚ñÑ‚ñÑ‚ñà‚ñà    ‚ñÄ‚ñà ‚ñà‚ñà    ‚ñÄ‚ñà  ‚ñà‚ñà   ‚ñÄ‚ñà‚ñà‚ñÑ 
  ‚ñà‚ñà   ‚ñÑ‚ñà‚ñà    ‚ñÑ‚ñà‚ñÄ‚ñà‚ñà‚ñÑ     ‚ñà‚ñà   ‚ñÑ‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñÑ     ‚ñà‚ñà   ‚ñà    ‚ñà‚ñà   ‚ñÑ‚ñà‚ñà  
  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñÑ‚ñà  ‚ñÄ‚ñà‚ñà     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   
  ‚ñà‚ñà         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚ñà‚ñà  ‚ñà‚ñà‚ñÑ  ‚ñÑ     ‚ñÄ‚ñà‚ñà ‚ñà‚ñà   ‚ñà  ‚ñÑ ‚ñà‚ñà  ‚ñà‚ñà‚ñÑ   
  ‚ñà‚ñà        ‚ñà‚ñÄ      ‚ñà‚ñà   ‚ñà‚ñà   ‚ñÄ‚ñà‚ñà‚ñÑ‚ñà‚ñà     ‚ñà‚ñà ‚ñà‚ñà     ‚ñÑ‚ñà ‚ñà‚ñà   ‚ñÄ‚ñà‚ñà‚ñÑ 
‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ    ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ   ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ
"""
    print(MENU)
    print("             ")
    print("         " + P)
    print("             ")
    

menu_load()

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
save_folder = '/storage/emulated/0/P.H.H.D'  # üëâ –ü–∞–ø–∫–∞ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ
os.makedirs(save_folder, exist_ok=True)

# === –í–í–û–î –ö–û–õ–ò–ß–ï–°–¢–í–ê –°–°–´–õ–û–ö ===
try:
    num_urls = int(input("–°–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å? "))
except ValueError:
    print("–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —á–∏—Å–ª–æ.")
    exit()

# === –í–í–û–î –°–°–´–õ–û–ö ===
urls = []
for i in range(num_urls):
    u = input(f"[{i+1}] –í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É: ").strip()
    urls.append(u)

# === –§–£–ù–ö–¶–ò–Ø –°–ö–ê–ß–ò–í–ê–ù–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô ===
def download_images_from_url(url, folder, base_count=0):
    print(f"\n[*] –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
    try:
        response = requests.get(url, timeout=10)
    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
        return 0

    soup = BeautifulSoup(response.text, "html.parser")

    # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    resources = []
    for tag in soup.find_all("img"):
        src = tag.get("src")
        if src:
            resources.append(src)

    print(f"[*] –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(resources)}")

    count = 0
    allowed_exts = [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".heic", ".heif", ".tiff", ".tif", ".raw", ".psd", ".ico", ".dds", ".apng", ".tga", ".xcf"]

    for res in resources:
        file_url = urljoin(url, res)
        parsed = urlparse(file_url)
        ext = os.path.splitext(parsed.path)[1].lower()

        if ext not in allowed_exts:
            print(f"[!] –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ): {file_url}")
            continue

        try:
            print(f"[>] –°–∫–∞—á–∏–≤–∞–µ–º: {file_url}")
            file_data = requests.get(file_url, timeout=5).content
            filename = os.path.join(folder, f"image_{base_count + count + 1}{ext}")
            with open(filename, "wb") as f:
                f.write(file_data)
            print(f"[+] –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
            count += 1
        except Exception as e:
            print(f"[!] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_url}: {e}")

    return count

# === –û–ë–†–ê–ë–û–¢–ö–ê –í–°–ï–• –°–°–´–õ–û–ö ===
total_downloaded = 0
for index, link in enumerate(urls):
    total_downloaded += download_images_from_url(link, save_folder, base_count=total_downloaded)

print(f"\n[‚úì] –ì–æ—Ç–æ–≤–æ! –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_downloaded}")